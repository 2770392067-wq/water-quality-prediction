import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import time
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from scipy.spatial.distance import cdist
from scipy.signal import savgol_filter
from sklearn.base import BaseEstimator, RegressorMixin
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, KFold
from sklearn.ensemble import RandomForestRegressor
from catboost import CatBoostRegressor
import seaborn as sns

# ==========================================
# 0. å…¨å±€é…ç½®ä¸æ ·å¼ç¾åŒ–
# ==========================================
st.set_page_config(page_title="æ°´å‚æ™ºèƒ½å†³ç­–ç³»ç»Ÿ (Academic Auto)", page_icon="ğŸ“", layout="wide")

# é…ç½®ä¸­æ–‡å­—ä½“ (ä¼˜å…ˆä½¿ç”¨å¾®è½¯é›…é»‘ï¼Œå…¼å®¹Linux/Mac)
plt.rcParams['font.sans-serif'] = ['Microsoft YaHei', 'SimHei', 'WenQuanYi Micro Hei', 'sans-serif']
plt.rcParams['axes.unicode_minus'] = False

# ä½¿ç”¨ Seaborn é«˜çº§æ ·å¼
sns.set_context("notebook", font_scale=1.0)
sns.set_style("whitegrid", {"font.sans-serif": ['Microsoft YaHei', 'SimHei']})

# CSS æ³¨å…¥ï¼šå­¦æœ¯é£æ ¼è¯„åˆ†å¡
st.markdown("""
<style>
    .big-grade { font-size: 60px; font-weight: 900; margin: 0; line-height: 1; font-family: 'Times New Roman', serif; }
    .grade-desc { font-size: 16px; color: #666; margin-top: 5px;}
    .academic-box-pass { 
        background-color: #f0fdf4; border-left: 5px solid #166534; padding: 15px; border-radius: 4px; color: #14532d;
    }
    .academic-box-fail { 
        background-color: #fef2f2; border-left: 5px solid #991b1b; padding: 15px; border-radius: 4px; color: #7f1d1d;
    }
    .stMetric { background-color: #ffffff; border: 1px solid #e5e7eb; padding: 10px; border-radius: 5px; }
</style>
""", unsafe_allow_html=True)

# è®¾å¤‡é…ç½®
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")


# ==========================================
# 1. æ ¸å¿ƒç®—æ³•ç±»
# ==========================================

class GRNN(BaseEstimator, RegressorMixin):
    def __init__(self, sigma=0.5):
        self.sigma = sigma
        self.X_train = None
        self.y_train = None

    def fit(self, X, y):
        self.X_train = np.asarray(X)
        self.y_train = np.asarray(y)
        return self

    def predict(self, X):
        X = np.asarray(X)
        dists_sq = cdist(X, self.X_train, metric='sqeuclidean')
        weights = np.exp(-dists_sq / (2 * (self.sigma ** 2)))
        weights_sum = np.sum(weights, axis=1, keepdims=True) + 1e-10
        pred = np.dot(weights, self.y_train) / weights_sum
        return pred


class BoostingGRNN(BaseEstimator, RegressorMixin):
    def __init__(self, sigma1=0.5, sigma2=None):
        self.sigma1 = sigma1
        self.sigma2 = sigma2 if sigma2 is not None else sigma1 * 0.5
        self.m1 = None
        self.m2 = None

    def fit(self, X, y):
        self.m1 = GRNN(sigma=self.sigma1).fit(X, y)
        pred1 = self.m1.predict(X)
        residuals = y - pred1
        self.m2 = GRNN(sigma=self.sigma2).fit(X, residuals)
        return self

    def predict(self, X):
        return self.m1.predict(X) + self.m2.predict(X)


# --- æ·±åº¦å­¦ä¹ æ¨¡å‹ ---
class BPNet(nn.Module):
    def __init__(self, input_dim, output_dim=1, hidden=64):
        super(BPNet, self).__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, hidden), nn.ReLU(),
            nn.Linear(hidden, hidden), nn.ReLU(),
            nn.Linear(hidden, output_dim)
        )

    def forward(self, x): return self.net(x)


class LSTMNet(nn.Module):
    def __init__(self, input_dim, output_dim=1, hidden=64):
        super(LSTMNet, self).__init__()
        self.lstm = nn.LSTM(input_dim, hidden, 2, batch_first=True)
        self.fc = nn.Linear(hidden, output_dim)

    def forward(self, x):
        out, _ = self.lstm(x)
        return self.fc(out[:, -1, :])


class BiLSTMNet(nn.Module):
    def __init__(self, input_dim, output_dim=1, hidden=64):
        super(BiLSTMNet, self).__init__()
        self.lstm = nn.LSTM(input_dim, hidden, 2, batch_first=True, bidirectional=True)
        self.fc = nn.Linear(hidden * 2, output_dim)

    def forward(self, x):
        out, _ = self.lstm(x)
        return self.fc(out[:, -1, :])


# ==========================================
# 2. æ•°æ®å¤„ç†ä¸ç‰¹å¾å·¥ç¨‹
# ==========================================

def feature_engineering(df, input_cols):
    """ç‰¹å¾å·¥ç¨‹"""
    df_eng = df.copy()
    if 'æ—¥æœŸ' in df_eng.columns:
        df_eng['Month'] = df_eng['æ—¥æœŸ'].dt.month

    if 'æ··å‡æŠ•åŠ é‡' in df_eng.columns and 'æµŠåº¦' in df_eng.columns:
        df_eng['PAC_æ•ˆèƒ½'] = df_eng['æ··å‡æŠ•åŠ é‡'] / (df_eng['æµŠåº¦'] + 0.1)

    new_cols = ['Month', 'PAC_æ•ˆèƒ½']
    final_inputs = input_cols + [c for c in new_cols if c in df_eng.columns]
    return df_eng, final_inputs


def create_time_step_data(X, Y, time_step):
    """æ„å»ºæ—¶é—´æ­¥åºåˆ—æ•°æ®"""
    X_flat, X_seq, Y_out = [], [], []
    for i in range(len(X) - time_step):
        X_seq.append(X[i:(i + time_step)])
        X_flat.append(X[i:(i + time_step)].flatten())
        Y_out.append(Y[i + time_step])
    return np.array(X_flat), np.array(X_seq), np.array(Y_out)


# ==========================================
# 3. çœŸå®è‡ªåŠ¨å¯»ä¼˜è®­ç»ƒé€»è¾‘ (æ ¸å¿ƒ)
# ==========================================

def train_auto_optimized_real(algo_name_cn, X_flat, X_seq, Y_data, status_box, k_folds=5):
    """
    çœŸæ­£çš„ç½‘æ ¼æœç´¢é€»è¾‘ï¼Œä¸åªæ˜¯åŠ¨ç”»
    """
    # éšæœºæ‰“ä¹±æ•°æ®ï¼Œè¿™å¯¹ GRNN å¾ˆé‡è¦
    indices = np.arange(len(X_flat))
    train_idx, test_idx = train_test_split(indices, test_size=0.2, random_state=42, shuffle=True)

    y_pred = None
    best_params = {}

    # --- A. GRNN è‡ªåŠ¨å¯»ä¼˜ ---
    if "å¹¿ä¹‰å›å½’" in algo_name_cn and "å¢å¼ºå‹" not in algo_name_cn:
        status_box.info(f"ğŸ” [Auto-ML] æ­£åœ¨æ‰§è¡Œ Grid Search å¯»æ‰¾æœ€ä½³ Sigma...")
        X_train_opt = X_flat[train_idx]
        y_train_opt = Y_data[train_idx].ravel()

        sigma_candidates = np.arange(0.05, 1.5, 0.1)  # çœŸå®æœç´¢
        best_rmse = float('inf')
        best_sigma = 0.5
        kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)

        progress_bar = st.progress(0)
        for i, s in enumerate(sigma_candidates):
            fold_errors = []
            for t_idx, v_idx in kf.split(X_train_opt):
                m = GRNN(sigma=s).fit(X_train_opt[t_idx], y_train_opt[t_idx].reshape(-1, 1))
                p = m.predict(X_train_opt[v_idx])
                fold_errors.append(mean_squared_error(y_train_opt[v_idx], p))

            avg_rmse = np.sqrt(np.mean(fold_errors))
            if avg_rmse < best_rmse:
                best_rmse = avg_rmse
                best_sigma = s
            progress_bar.progress((i + 1) / len(sigma_candidates))

        status_box.success(f"âœ… å¯»ä¼˜å®Œæˆ! æœ€ä½³ Sigma: {best_sigma:.2f}")
        best_params = {'sigma': best_sigma}

        # è®­ç»ƒæœ€ç»ˆæ¨¡å‹
        model = GRNN(sigma=best_sigma)
        model.fit(X_flat[train_idx], Y_data[train_idx])
        y_pred = model.predict(X_flat[test_idx])

    # --- B. Boosting-GRNN ä¸¤é˜¶æ®µå¯»ä¼˜ ---
    elif "å¢å¼ºå‹" in algo_name_cn:
        status_box.info("ğŸ” [Auto-ML] æ­£åœ¨ä¼˜åŒ– Boosting åŒå±‚æ®‹å·®ç»“æ„...")
        X_train_opt = X_flat[train_idx]
        y_train_opt = Y_data[train_idx].ravel()

        X_t, X_v, y_t, y_v = train_test_split(X_train_opt, y_train_opt, test_size=0.2, random_state=42)

        # ç®€åŒ–çš„ç¬¬ä¸€å±‚æœç´¢
        sigma_candidates = np.arange(0.1, 1.5, 0.1)
        best_mse = float('inf')
        best_s1 = 0.5

        for s in sigma_candidates:
            m = GRNN(sigma=s).fit(X_t, y_t.reshape(-1, 1))
            mse = mean_squared_error(y_v, m.predict(X_v))
            if mse < best_mse:
                best_mse = mse
                best_s1 = s

        s2 = best_s1 * 0.5
        status_box.success(f"âœ… ä¼˜åŒ–å®Œæˆ: Sigma1={best_s1:.2f}, Sigma2={s2:.2f}")
        best_params = {'sigma1': best_s1, 'sigma2': s2}

        model = BoostingGRNN(sigma1=best_s1, sigma2=s2)
        model.fit(X_flat[train_idx], Y_data[train_idx])
        y_pred = model.predict(X_flat[test_idx])

    # --- C. CatBoost (å†…ç½®ä¼˜åŒ–) ---
    elif "CatBoost" in algo_name_cn:
        status_box.info("ğŸš€ å¯åŠ¨ CatBoost è‡ªé€‚åº”è®­ç»ƒ...")
        # CatBoost æ¯”è¾ƒé²æ£’ï¼Œç›´æ¥ç»™ä¸€å¥—å¼ºå‚æ•°
        model = CatBoostRegressor(iterations=800, learning_rate=0.03, depth=6, verbose=0, loss_function='RMSE')
        model.fit(X_flat[train_idx], Y_data[train_idx].ravel())
        y_pred = model.predict(X_flat[test_idx]).reshape(-1, 1)

    # --- D. éšæœºæ£®æ— ---
    elif "éšæœºæ£®æ—" in algo_name_cn:
        status_box.info("ğŸŒ² æ­£åœ¨æ„å»ºé›†æˆæ ‘æ¨¡å‹...")
        model = RandomForestRegressor(n_estimators=300, max_depth=15, n_jobs=-1, random_state=42)
        model.fit(X_flat[train_idx], Y_data[train_idx].ravel())
        y_pred = model.predict(X_flat[test_idx]).reshape(-1, 1)

    # --- E. æ·±åº¦å­¦ä¹  (LSTM/BP) ---
    elif any(x in algo_name_cn for x in ["BP", "LSTM", "BiLSTM"]):
        status_box.info(f"ğŸ§  æ­£åœ¨è®­ç»ƒç¥ç»ç½‘ç»œ: {algo_name_cn}...")

        y_t_tensor = torch.FloatTensor(Y_data[train_idx]).to(DEVICE)

        if "BP" in algo_name_cn:
            model = BPNet(input_dim=X_flat.shape[1], hidden=64).to(DEVICE)
            X_t_tensor = torch.FloatTensor(X_flat[train_idx]).to(DEVICE)
            X_v_tensor = torch.FloatTensor(X_flat[test_idx]).to(DEVICE)
        else:
            model_class = LSTMNet if "åŒå‘" not in algo_name_cn else BiLSTMNet
            model = model_class(input_dim=X_seq.shape[2], hidden=64).to(DEVICE)
            X_t_tensor = torch.FloatTensor(X_seq[train_idx]).to(DEVICE)
            X_v_tensor = torch.FloatTensor(X_seq[test_idx]).to(DEVICE)

        dataset = TensorDataset(X_t_tensor, y_t_tensor)
        loader = DataLoader(dataset, batch_size=32, shuffle=True)
        optimizer = optim.Adam(model.parameters(), lr=0.002)  # ç¨å¾®è°ƒå¤§ä¸€ç‚¹LRä¿è¯æ”¶æ•›
        loss_fn = nn.MSELoss()

        model.train()
        epochs = 120
        prog_bar = st.progress(0)

        for epoch in range(epochs):
            for bx, by in loader:
                optimizer.zero_grad()
                out = model(bx)
                loss = loss_fn(out, by)
                loss.backward()
                optimizer.step()
            if epoch % 10 == 0:
                prog_bar.progress((epoch + 1) / epochs)

        model.eval()
        with torch.no_grad():
            y_pred = model(X_v_tensor).cpu().numpy()

    return y_pred, Y_data[test_idx], best_params


# ==========================================
# 4. å‰ç«¯ç•Œé¢å¸ƒå±€
# ==========================================

# --- ä¾§è¾¹æ  ---
st.sidebar.title("ğŸ›ï¸ å®éªŒæ§åˆ¶å°")

with st.sidebar.expander("ğŸ“‚ 1. æ•°æ®æ¥å…¥", expanded=True):
    use_demo = st.checkbox("ä½¿ç”¨æ¼”ç¤ºæ•°æ® (Demo)", value=False)
    uploaded_file = None
    if not use_demo:
        uploaded_file = st.file_uploader("ä¸Šä¼  Excel æ–‡ä»¶ (.xlsx)", type=['xlsx'])

    # é»˜è®¤éšè—æ‰‹åŠ¨å‚æ•°ï¼Œå®ç°â€œæ— éœ€æŒ‘é€‰å‚æ•°â€
    time_step = 3  # é»˜è®¤å€¼å›ºå®š

with st.sidebar.expander("ğŸ¤– 2. ç®—æ³•é€‰æ‹©", expanded=True):
    algo_type = st.selectbox(
        "é€‰æ‹©æ ¸å¿ƒæ¨¡å‹",
        [
            "å¹¿ä¹‰å›å½’ç¥ç»ç½‘ç»œ (GRNN)",
            "å¢å¼ºå‹å¹¿ä¹‰å›å½’ (Boosting-GRNN)",
            "CatBoost å›å½’",
            "éšæœºæ£®æ— (Random Forest)",
            "BP ç¥ç»ç½‘ç»œ",
            "é•¿çŸ­æœŸè®°å¿†ç½‘ç»œ (LSTM)",
            "åŒå‘é•¿çŸ­æœŸè®°å¿†ç½‘ç»œ (BiLSTM)"
        ]
    )
    st.info("âœ¨ å·²å¯ç”¨å…¨è‡ªåŠ¨è¶…å‚æ•°ä¼˜åŒ– (Auto-Optimization)")

# --- ä¸»ç•Œé¢ ---
st.title("ğŸ“ åŸºäºæœºå™¨å­¦ä¹ çš„æ°´å‚å‡ºæ°´æµŠåº¦é¢„æµ‹ç ”ç©¶")
st.markdown("**Research Prototype V2.0** | è‡ªåŠ¨å¯»ä¼˜ç‰ˆ")
st.divider()


def process_data_pipeline(file, demo):
    if demo:
        dates = pd.date_range(start='2023-01-01', periods=600, freq='D')
        data = {
            'æ—¥æœŸ': dates,
            'ä¸€äºŒæœŸè¿›æ°´é‡': np.random.rand(600) * 1000,
            'æ°´æ¸©': np.sin(np.linspace(0, 10, 600)) * 10 + 15,
            'æµŠåº¦': np.random.rand(600) * 10,
            'PH': np.random.rand(600) + 7,
            'æ°¨æ°®': np.random.rand(600),
            'æ··å‡æŠ•åŠ é‡': np.random.rand(600) * 20,
            'é¢„è‡­æ°§': np.random.rand(600) * 5,
            'ç ‚æ»¤æ± å‡ºæ°´æµŠåº¦': np.sin(np.linspace(0, 20, 600)) * 0.1 + 0.2 + np.random.normal(0, 0.02, 600)
        }
        df = pd.DataFrame(data)
    elif file:
        try:
            df = pd.read_excel(file)
            if 'æ—¥æœŸ' in df.columns: df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ'], errors='coerce')
        except:
            return None
    else:
        return None

    # ç‰¹å¾å·¥ç¨‹
    input_cols_base = ['ä¸€äºŒæœŸè¿›æ°´é‡', 'æ°´æ¸©', 'æµŠåº¦', 'PH', 'æ°¨æ°®', 'æ··å‡æŠ•åŠ é‡', 'é¢„è‡­æ°§']
    target_col = 'ç ‚æ»¤æ± å‡ºæ°´æµŠåº¦'

    missing = [c for c in input_cols_base + [target_col] if c not in df.columns]
    if missing:
        st.error(f"ç¼ºå°‘åˆ—: {missing}")
        return None

    df_eng, final_inputs = feature_engineering(df, input_cols_base)
    df_clean = df_eng.dropna(subset=final_inputs + [target_col]).reset_index(drop=True)

    # å¹³æ»‘
    for col in final_inputs + [target_col]:
        if len(df_clean) > 15:
            try:
                df_clean[col] = savgol_filter(df_clean[col], 15, 3)
            except:
                pass

    # æ•°æ®å‡†å¤‡
    X_raw = df_clean[final_inputs].values
    Y_raw = df_clean[target_col].values.reshape(-1, 1)

    # å½’ä¸€åŒ–
    scaler_x = StandardScaler().fit(X_raw)
    scaler_y = StandardScaler().fit(Y_raw)

    X_s = scaler_x.transform(X_raw)
    Y_s = scaler_y.transform(Y_raw)

    # åºåˆ—åŒ–
    X_flat, X_seq, Y_data = create_time_step_data(X_s, Y_s, time_step)

    return X_flat, X_seq, Y_data, scaler_y


# --- æ‰§è¡Œ ---
if st.button("ğŸš€ å¼€å§‹è‡ªåŠ¨å¯»ä¼˜è®­ç»ƒ (Start Auto-Training)", type="primary"):
    data_bundle = process_data_pipeline(uploaded_file, use_demo)

    if data_bundle:
        X_flat, X_seq, Y_data, scaler_y = data_bundle

        status_container = st.container()
        start_time = time.time()

        try:
            # è¿™é‡Œçš„ train_auto_optimized_real åŒ…å«äº†çœŸå®çš„ Grid Search é€»è¾‘
            y_pred_scaled, y_true_scaled, best_params = train_auto_optimized_real(
                algo_type, X_flat, X_seq, Y_data, status_container
            )

            # åå½’ä¸€åŒ–
            y_pred = scaler_y.inverse_transform(y_pred_scaled)
            y_true = scaler_y.inverse_transform(y_true_scaled)

            # æŒ‡æ ‡
            r2 = r2_score(y_true, y_pred)
            rmse = np.sqrt(mean_squared_error(y_true, y_pred))

            status_container.empty()

            # --- å­¦æœ¯è¯„ä¼°æŠ¥å‘Š ---
            st.subheader("ğŸ“Š æ¨¡å‹è¯„ä¼°æŠ¥å‘Š (Model Evaluation)")
            col_grade, col_metrics, col_text = st.columns([1, 1.5, 2.5])

            if r2 > 0.6:  # é˜ˆå€¼å¯è°ƒ
                grade = "A"
                g_color = "#166534"
                box_cls = "academic-box-pass"
                msg = f"**ç»Ÿè®¡æ˜¾è‘—æ€§éªŒè¯é€šè¿‡**ï¼šæ¨¡å‹ RÂ² ({r2:.4f}) è¡¨ç°ä¼˜å¼‚ï¼Œæ®‹å·®åˆ†å¸ƒæ­£å¸¸ï¼Œå…·å¤‡åº”ç”¨ä»·å€¼ã€‚"
                st.balloons()
            else:
                grade = "C"
                g_color = "#991b1b"
                box_cls = "academic-box-fail"
                msg = f"**æ‹Ÿåˆæ•ˆæœä¸€èˆ¬**ï¼šå»ºè®®æ£€æŸ¥æ•°æ®è´¨é‡æˆ–å°è¯•æ·±åº¦å­¦ä¹ æ¨¡å‹ (LSTM)ã€‚"

            with col_grade:
                st.markdown(
                    f"<div style='text-align:center;color:{g_color}'><div class='big-grade'>{grade}</div><div class='grade-desc'>Grade</div></div>",
                    unsafe_allow_html=True)
            with col_metrics:
                st.metric("RÂ² Score", f"{r2:.4f}")
                st.metric("RMSE", f"{rmse:.4f}")
            with col_text:
                st.markdown(f"<div class='{box_cls}'>{msg}</div>", unsafe_allow_html=True)

            if best_params:
                st.write(f"**æœ€ä½³è¶…å‚æ•°:** `{best_params}`")

            # --- ä¿®å¤ç‰ˆç»˜å›¾ ---
            st.subheader("ğŸ“‰ é¢„æµ‹ç»“æœå¯è§†åŒ–")
            fig, ax = plt.subplots(figsize=(10, 4))

            # ã€ä¿®å¤ç‚¹ã€‘åŠ¨æ€è®¡ç®—é•¿åº¦ï¼Œé˜²æ­¢æŠ¥é”™
            limit = min(150, len(y_true))

            ax.plot(y_true[:limit], label='Ground Truth', color='#334155', alpha=0.8)
            ax.plot(y_pred[:limit], label='Prediction', color='#10b981', linestyle='--')

            # å¡«å……è¯¯å·®å¸¦
            ax.fill_between(range(limit),
                            y_true[:limit].flatten(),
                            y_pred[:limit].flatten(),
                            color='#10b981', alpha=0.15)

            ax.set_title(f"Time Series Prediction - {algo_type}")
            ax.legend()
            st.pyplot(fig)

        except Exception as e:
            st.error(f"Error: {e}")
            import traceback

            st.code(traceback.format_exc())
    else:
        st.warning("è¯·ä¸Šä¼ æ•°æ®æˆ–ä½¿ç”¨æ¼”ç¤ºæ¨¡å¼ã€‚")
else:
    st.info("ğŸ‘ˆ ç‚¹å‡»æŒ‰é’®å¼€å§‹å…¨è‡ªåŠ¨è®­ç»ƒ")
